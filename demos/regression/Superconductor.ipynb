{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f249a8",
   "metadata": {},
   "source": [
    "# Regression Demo on Superconductor Dataset\n",
    "*Work produced by Stephen Toner, Fall 2021*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea8510",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as os\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import itertools\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d20872",
   "metadata": {},
   "source": [
    "We are analyzing data from the University of California Irvine\n",
    "[superconductor](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data)\n",
    "dataset that includes the physical properties of different superconductors,\n",
    "one of which is the critical point of the superconductor. We wish to estimate\n",
    "the critical point of a substance based on its other physical properties \n",
    "using three different machine learning methods: Elastic Net, \n",
    "Random Forests, and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69886f15",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a72edf",
   "metadata": {},
   "source": [
    "We begin by reading the data and segmenting it into training, validation,\n",
    "and testing subsets,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "x_total = train.iloc[:,:-1]\n",
    "y_total = train.iloc[:,-1]\n",
    "\n",
    "n_samples = len(y_total) # \n",
    "\n",
    "idx = np.arange(len(y_total))\n",
    "\n",
    "# Initialize random number generator\n",
    "rng = np.random.default_rng(11 * 19 + 2021)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "n_train = int(n_samples * .8 // 1)\n",
    "n_test = int(n_samples * .1 // 1)\n",
    "n_val = n_samples - n_test - n_train\n",
    "\n",
    "train_idx = idx[0:n_train]\n",
    "test_idx = idx[n_train:(n_train + n_test)]\n",
    "val_idx = idx[(n_train + n_test):(n_train + n_test+n_val)]\n",
    "\n",
    "x_train = x_total.iloc[train_idx].to_numpy()\n",
    "y_train = y_total.iloc[train_idx].to_numpy()\n",
    "\n",
    "x_test = x_total.iloc[test_idx].to_numpy()\n",
    "y_test = y_total.iloc[test_idx].to_numpy()\n",
    "\n",
    "x_val = x_total.iloc[val_idx].to_numpy()\n",
    "y_val = y_total.iloc[val_idx].to_numpy()\n",
    "\n",
    "folds = np.array_split(np.arange(len(x_val)),10) # Previously randomized, no need to shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab5e26",
   "metadata": {},
   "source": [
    "## Training and Tuning Models\n",
    "\n",
    "As we train and tune the first pass of the models using a cross validation /\n",
    "grid search approach, we will use the functions below to explore different \n",
    "combinations of hyperparameters. As this can take some time, we will save the\n",
    "resulting errors for each model to the working directory so we can quickly\n",
    "load it again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb3fcc",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def param_product(**kwargs):\n",
    "    \"\"\"\n",
    "    Returns a generator of dictionaries for all combinations of values \n",
    "    for each tuning parameter passed. For instance, if you were\n",
    "    to pass C = [1, 5] and max_iter = [1, 10], the generator would \n",
    "    return dicts of {C: 1, max_iter : 1}, {C: 1, max_iter : 10}, etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    **kwargs : Dict of Lists\n",
    "        A dictionary where each key represents a tuning parameter, and\n",
    "        the corresponding value is a list of values to set for the\n",
    "        tuning parameters\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Generator of dictionaries\n",
    "        Each dictionary can subsequently be passed to the sklearn\n",
    "        model's **kwargs parameter\n",
    "\n",
    "    \"\"\"    \n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for elt in itertools.product(*vals):\n",
    "        yield dict(zip(keys,elt))\n",
    "    \n",
    "        \n",
    "def gen_label(dict_item):\n",
    "    \"\"\"\n",
    "    Helper function for param_product. creates a unique string to\n",
    "    act as the key for a given combination of tuning parameters\n",
    "    to be stored in the errors / avg_errors dictionary in\n",
    "    cross_validate\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_item : \n",
    "        A single dictionary returned from the param_product\n",
    "        generator. Should only have one parameter value for each key\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    String\n",
    "        Unique space-delimited string for the given combination of \n",
    "        tuning parameters / values\n",
    "    \"\"\"\n",
    "    label = []\n",
    "    for k, v in dict_item.items():\n",
    "        for i in[k, str(v)]:\n",
    "            label.append(i)\n",
    "    return \" \".join(label)\n",
    "    \n",
    "def cross_validate(name, k_folds, param_space, train, labels):\n",
    "    \"\"\"\n",
    "    Performs k-fold cross validation across param_space\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string\n",
    "        Model to be used for cross validtion. Can be \n",
    "          - \"en\" => ElasticNet\n",
    "          - \"rfr\" => RandomForestRegressor\n",
    "          - \"gbr\" => GradientBoostingRegressor\n",
    "    k_folds : \n",
    "        List of lists, each corresponding ot the indices of the different\n",
    "        folds in train with which to perform cross validation. Should already\n",
    "        be randomized before function call\n",
    "    param_space : Dictionary\n",
    "        Dictionary where keys correspond to tuning parameters, and values\n",
    "        are lists of values to set for each pass of cross validation\n",
    "    train : ndarray\n",
    "        training data with which to perform cross validation\n",
    "    labels : ndarray (vector, n x 1)\n",
    "        labels for training data to measure mse\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    avg_errors() (list of tuples)\n",
    "        unpacks results from cross validation, returning \n",
    "        a list of tuples where the first value is the unique\n",
    "        string of model training parameters, and the second value\n",
    "        is the resulting MSE. Sorts in ascending order by loss measure\n",
    "\n",
    "    \"\"\"\n",
    "    errors = {}\n",
    "    avg_errors = {}\n",
    "    loss = mse\n",
    "    params = param_product(**param_space)\n",
    "    for p in params:\n",
    "\n",
    "        p_label = gen_label(p)\n",
    "        errors[p_label] = []\n",
    "        if name == \"en\": # Elastic Net\n",
    "            model = en(max_iter = 50000, tol=.001, warm_start=True, **p)\n",
    "        elif name ==\"rf\": # Random Forest\n",
    "            model =rfr(n_jobs = 2, **p)\n",
    "        elif name ==\"gbr\": # Gradient Boosted Regressor\n",
    "            model = gbr(**p)\n",
    "        else:\n",
    "            break\n",
    "        for k in k_folds:\n",
    "            # Segment out the validation set\n",
    "            mask = np.ones(len(train), dtype=bool)\n",
    "            mask[k] = False\n",
    "            i_train = train[mask]\n",
    "            i_tr_labels = labels[mask]\n",
    "            i_test = train[k]\n",
    "            i_t_labels = labels[k]\n",
    "            # Train and Estimate holdout error\n",
    "            model_f = model.fit(i_train, i_tr_labels)\n",
    "            pred = model_f.predict(i_test)\n",
    "            errors[p_label].append(loss(i_t_labels, pred))\n",
    "            \n",
    "        avg_errors[p_label] = np.mean(errors[p_label])\n",
    "    return sorted(list(avg_errors.items()),key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99891cf",
   "metadata": {},
   "source": [
    "### Elastic Net ### \n",
    "Elastic Net is a hybrid between two of the most popular regularized\n",
    "regression methods in machine learning, ridge regression and \n",
    "lasso regression. The fundamental approach for both methods is the same,\n",
    "but they differ in their regularization term.\n",
    "\n",
    "The form of ridge regression is \n",
    "$$ F_{ridge}(\\mathbf{w}) = \\sum ^n _{j = 1} (y_j - \\hat y_j (\\mathbf{w}))^2 + \n",
    "\\lambda \\sum_{i=1}^p \\|w_i\\|_2^2 $$\n",
    "\n",
    "While lasso regression has the form \n",
    "$$ F_{lasso}(\\mathbf{w}) = \\sum ^n _{j = 1} (y_j - \\hat y_j (\\mathbf{w}))^2 + \n",
    "\\lambda \\sum_{i=1}^p |w_i|_1 $$\n",
    "\n",
    "Elastic Net uses a linear combination of the L2 and L1 penalties used\n",
    "by ridge and lasso regression, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3709006",
   "metadata": {},
   "source": [
    "The two parameters we will be sensitizing over are alpha and l1_ratio:\n",
    "+ alpha: Coefficient for the penalty terms, determines how much the model\n",
    "tries to keep the magnitude of the weights small\n",
    "+ l1_ratio: Specifies the proportion of the penalty terms that are L1 vs. L2\n",
    "Below we conduct a grid search, where we have a range of values for each\n",
    "hyperparameter and explore all combinations of values for each hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930146d8",
   "metadata": {},
   "source": [
    "The ranges for the grid search below were based on trial-and-error to see\n",
    "that a given set of inputs did not result in weak duality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd64ded",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# alpha \n",
    "alpha_space = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "# l1_ratio\n",
    "l1_space = np.linspace(0,1,5)[1:] # Omit zero\n",
    "\n",
    "en_parameters = {\n",
    "            \"alpha\" : alpha_space,\n",
    "            \"l1_ratio\" : l1_space\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21774d2b",
   "metadata": {},
   "source": [
    "Now we perfrom 10-fold cross validation, a process where we omit one tenth\n",
    "of the data at a time (the omitted data is called a fold), train the \n",
    "algorithm on the remaining  90% of the data, and then evaluate against the\n",
    "omitted fold. This helps us prevent overfitting to some specific feature or\n",
    "to a specific cohort of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4dbf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_filename = \"EnetErrors.pkl\"\n",
    "\n",
    "if(os.exists(en_filename)):\n",
    "    en_df = pd.read_pickle(en_filename)\n",
    "else:\n",
    "    en_errors = cross_validate(\"en\", folds, en_parameters, x_train, y_train)\n",
    "    en_df = pd.DataFrame(en_errors, columns =[\"String\", \"MSE\"])\n",
    "    en_df[\"alpha\"] = en_df[\"String\"].str.split(\"alpha|l1_ratio\", expand=True)[1]\n",
    "    en_df[\"l1\"] = en_df[\"String\"].str.split(\"alpha|l1_ratio\", expand=True)[2]\n",
    "    en_df.to_pickle(en_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for k, g in en_df.groupby([\"l1\"]):\n",
    "    ax.plot(g[\"alpha\"], g[\"MSE\"], label = k)\n",
    "\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"Mean Squared Error\")\n",
    "ax.set_title(\"MSE vs. Alpha by L1 Ratio\")\n",
    "plt.legend(loc='best', title =\"L1 Ratio\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "en_df = en_df.rename(columns={\"l1\": \"L1 Ratio\"})\n",
    "en_table = en_df.pivot(\"alpha\", \"L1 Ratio\", \"MSE\")\n",
    "\n",
    "display(HTML(en_table.to_html(index= True, \n",
    "                                 float_format = lambda x: \"%10.2f\" % x)))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536cb848",
   "metadata": {},
   "source": [
    "Comments: Generally speaking MSE is an increasing function of alpha and l1.\n",
    "This means that we have many features that we need to make use of in order\n",
    "to make a good prediction (low alpha) and that Ridge is outperforming Lasso\n",
    "regression in this particular case (low l1 ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42caf2",
   "metadata": {},
   "source": [
    "### Random Forest ###\n",
    "\n",
    "The name \"Random Forest\" comes from the fact that this method creates \n",
    "a \"forest\" of decision trees in order to predict an outcome, in this case\n",
    "a regression value. The use of many randomly selected decision trees helps\n",
    "mitigate the tendency for decision trees to overfit, and allows for greater\n",
    "model stability and accuracy.\n",
    "\n",
    "Tuning Parameters\n",
    "+ n_estimators: Number of trees in the forest\n",
    "+ max_depth: Maximum allowable depth for a given tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est_space = [10, 50, 100] \n",
    "depth_space = [3, 6, 9, 12]\n",
    "\n",
    "rfr_parameters = {\n",
    "            \"n_estimators\" : n_est_space,\n",
    "            \"max_depth\" : depth_space\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08bf7",
   "metadata": {},
   "source": [
    "Next we cross validate again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_filename = \"RfErrors.pkl\"\n",
    "\n",
    "if(os.exists(rfr_filename)):\n",
    "    rfr_df = pd.read_pickle(rfr_filename)\n",
    "else:\n",
    "    rfr_errors = cross_validate(\"rf\", folds, rfr_parameters, x_train, y_train)\n",
    "    rfr_df = pd.DataFrame(rfr_errors, columns =[\"String\", \"MSE\"])\n",
    "\n",
    "    rfr_df[\"n_estimators\"] = rfr_df[\"String\"].str.split(\"n_estimators|max_depth\", \n",
    "                                                  expand=True)[1]\n",
    "    rfr_df[\"max_depth\"] = rfr_df[\"String\"].str.split(\"n_estimators|max_depth\", \n",
    "                                                  expand=True)[2]\n",
    "    rfr_df.to_pickle(rfr_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_df[\"num_estimators\"] = pd.to_numeric(rfr_df[\"n_estimators\"])\n",
    "rfr_df[\"max_depth\"] = pd.to_numeric(rfr_df[\"max_depth\"])\n",
    "fig, ax = plt.subplots()\n",
    "for k, g in rfr_df.groupby([\"num_estimators\"]):\n",
    "    ax.plot(g[\"max_depth\"], g[\"MSE\"], label = k)\n",
    "ax.set_xlabel(\"Max Depth\")\n",
    "ax.set_ylabel(\"Mean Squared Error\")\n",
    "ax.set_title(\"MSE vs. Max Depth by Number of Trees\")\n",
    "plt.legend(loc='best', title =\"# of Trees\")\n",
    "plt.show()\n",
    "\n",
    "rfr_df = rfr_df.rename(columns={\"num_estimators\": \"Number of Trees\",\n",
    "                                \"max_depth\": \"Max Depth\"})\n",
    "rfr_table = rfr_df.pivot(\"Number of Trees\", \"Max Depth\", \"MSE\")\n",
    "\n",
    "display(HTML(rfr_table.to_html(index= True, \n",
    "                                 float_format = lambda x: \"%10.2f\" % x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8cc7d",
   "metadata": {},
   "source": [
    "Comments:\n",
    "+ The random forest method achieves (in general) a much lower MSE than\n",
    "the previous Elastic Net method\n",
    "+ Increasing the number of trees and the maximum depth of each tree \n",
    "improved model accuracy and reduced mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6060c",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression ###\n",
    "\n",
    "The last class of model we will use is an example of an ensemble method,\n",
    "where a base learner takes an input of weights corresponding to a set\n",
    "of learning models, and determines the weight of each model's output so as\n",
    "to approximately minimize empirical risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7e211",
   "metadata": {},
   "source": [
    "Parameters\n",
    "+ Boosting Rounds: Number boosting rounds (sub-models) to construct\n",
    "+ Learning Rate: Each model has progressively less and less impact on the \n",
    "final output; learning rate determines how much each subsequent model's \n",
    "influence is reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_space = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "n_est_space = [3, 10, 15, 50, 100]\n",
    "gbr_parameters = {\"learning_rate\" : learning_rate_space,\n",
    "                    \"n_estimators\" : n_est_space}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2bd37",
   "metadata": {},
   "source": [
    "A final round of cross validation for gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38788ef3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gbr_filename = \"GBRErrors.pkl\"\n",
    "\n",
    "if(os.exists(gbr_filename)):\n",
    "    gbr_df = pd.read_pickle(gbr_filename)\n",
    "else:\n",
    "    gbr_errors = cross_validate(\"gbr\", folds, gbr_parameters, x_train, y_train)\n",
    "    gbr_df = pd.DataFrame(gbr_errors, columns =[\"String\", \"MSE\"])\n",
    "\n",
    "    gbr_df[\"learning_rate\"] = gbr_df[\"String\"].str.split(\"learning_rate|n_estimators\", \n",
    "                                                  expand=True)[1]\n",
    "    gbr_df[\"n_estimators\"] = gbr_df[\"String\"].str.split(\"learning_rate|n_estimators\", \n",
    "                                                  expand=True)[2]\n",
    "    gbr_df.to_pickle(gbr_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1bacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_df[\"num_estimators\"] = pd.to_numeric(gbr_df[\"n_estimators\"])\n",
    "gbr_df[\"learning_rate\"] = pd.to_numeric(gbr_df[\"learning_rate\"])\n",
    "\n",
    "gbr_df = gbr_df.sort_values([\"num_estimators\", \"learning_rate\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for k, g in gbr_df.groupby([\"num_estimators\"]):\n",
    "    ax.plot(g[\"learning_rate\"], g[\"MSE\"], label = k)\n",
    "ax.set_xlabel(\"Learning Rate\")\n",
    "ax.set_ylabel(\"Mean Squared Error\")\n",
    "ax.set_title(\"MSE vs. Learning Rate by Boosting Rounds\")\n",
    "plt.legend(loc='best', title =\"Boosting Rounds\")\n",
    "plt.show()\n",
    "\n",
    "gbr_df = gbr_df.rename(columns={\"num_estimators\": \"Boosting Rounds\",\n",
    "                                \"learning_rate\" : \"Learning Rate\"})\n",
    "gbr_table = gbr_df.pivot(\"Boosting Rounds\", \"Learning Rate\", \"MSE\")\n",
    "\n",
    "display(HTML(gbr_table.to_html(index= True, \n",
    "                                 float_format = lambda x: \"%10.2f\" % x)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbaa2c5",
   "metadata": {},
   "source": [
    "As one might expect, increasing the number of boosting rounds lowered MSE.\n",
    "The best choice of learning rate was 0.4, which suggests that some of the\n",
    "models learned in the final boosting rounds were being discounted too much\n",
    "when learning rate was set to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9888461",
   "metadata": {},
   "source": [
    "## Validation and Testing ##\n",
    "After we've tuned our hyperparameters, we need to choose which one of these \n",
    "models will be used against the test set. We can achieve this by taking the\n",
    "hyperparameters with the best performance against the training data and\n",
    "see how models with these parameters perform on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Elastic Net\n",
    "opt_en = en(alpha=0.2,l1_ratio=.25, max_iter = 50000, tol=.001)\n",
    "opt_en_f = opt_en.fit(x_train,y_train)\n",
    "\n",
    "# Final Random Forest\n",
    "opt_rfr = rfr(100,max_depth = 12)\n",
    "opt_rfr_f = opt_rfr.fit(x_train,y_train)\n",
    "\n",
    "# Final Gradient Boosting Regressor\n",
    "opt_gbr = gbr(n_estimators = 100, learning_rate = 0.4)\n",
    "opt_gbr_f = opt_gbr.fit(x_train,y_train)\n",
    "\n",
    "# Evaluate three models against the validation dataset\n",
    "pred_v_en = opt_en_f.predict(x_val)\n",
    "pred_v_rfr = opt_rfr_f.predict(x_val)\n",
    "pred_v_gbr = opt_gbr_f.predict(x_val)\n",
    "\n",
    "val_err_en = mse(y_val, pred_v_en)\n",
    "val_err_rfr = mse(y_val, pred_v_rfr)\n",
    "val_err_gbr = mse(y_val, pred_v_gbr)\n",
    "\n",
    "# Table with output (MSE)\n",
    "models = [\"Elastic Net\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "errors = [val_err_en, val_err_rfr, val_err_gbr]\n",
    "\n",
    "v_table = pd.DataFrame(data = models)\n",
    "v_table = v_table.rename(columns={0:\"Model\"})\n",
    "v_table[\"MSE\"] = errors\n",
    "v_table = v_table.sort_values(\"MSE\")\n",
    "\n",
    "display(HTML(v_table.to_html(index= False, \n",
    "                                 float_format = lambda x: \"%10.2f\" % x)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc22f8",
   "metadata": {},
   "source": [
    "It seems that based on mean squared error alone, our random forest regression\n",
    "was the best learning method for predicting superconductivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd94fa1",
   "metadata": {},
   "source": [
    "Now for the moment of truth: assessing predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_t = opt_rfr_f.predict(x_test)\n",
    "final_error = mse(y_test, pred_t)\n",
    "output_msg = \"The final MSE for the best model on the test data is {0:0.1f}\"\n",
    "print(output_msg.format(final_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0ca80",
   "metadata": {},
   "source": [
    "Our model has a relatively low MSE, suggesting it is decently well trained."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
