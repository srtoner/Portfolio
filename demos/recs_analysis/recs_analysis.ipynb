{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a587ded",
   "metadata": {},
   "source": [
    "# RECS Survey and Replicate Weights\n",
    "*Work produced by Stephen Toner*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e815a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as os\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3aba1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This analysis uses data from the Residential Energy Consumption Survey (RECS) 2009 and 2015 survey\n",
    "data which we will be analyzing can be downloaded here:\n",
    "* [2009 Data][1] \n",
    "* [2009 Replicate Weights][2]\n",
    "* [2015 Data][3]\n",
    "\n",
    "[1]: https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv\n",
    "[2]: https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public_repweights.csv\n",
    "[3]: https://www.eia.gov/consumption/residential/data/2015/csv/recs2015_public_v4.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05117e0",
   "metadata": {},
   "source": [
    "We wish to estimate the average number of heating and cooling\n",
    "degree days for residences by census region for 2009 and 2015.\n",
    "For reference, a heating or cooling degree day is a measure of how much \n",
    "colder or hotter, respectively, the ambient temperature is than the\n",
    "threshold temperature for comfort, in this case $65 ^\\circ F$. As an example,\n",
    "if the temperature outside is $45 ^\\circ F$, on a given day, this would \n",
    "be recorded as 20 heating degree days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4ecc2",
   "metadata": {},
   "source": [
    "In order to conduct our estimate (and provide appropriate confidence\n",
    "intervals), we will need the following variables from the RECS data:\n",
    "* DOEID: Department of Energy Identification Number, a unique key for each\n",
    "household surveyed\n",
    "* REGIONC: Census region of the household \n",
    "* HDD65: The number of heating degree days for the survey year\n",
    "* CDD65: The number of cooling degree days for the survey year\n",
    "* NWEIGHT: The final survey weight for each household in the sample; this\n",
    "corresponds with the number of other similar households a household in the \n",
    "survey sample represents\n",
    "In addition, we will also need to make use of the replicate weights to \n",
    "estimate the standard error as the RECS survey uses the balanced repeated\n",
    "replication ([BRR][4]) technique. These weights are included in the 2015\n",
    "RECS survey microdata file, but are held in a separate file for the 2009 \n",
    "survey.\n",
    "[4]: https://en.wikipedia.org/wiki/Balanced_repeated_replication\n",
    "\n",
    "A more detailed explanation of the BRR technique used in the RECS survey, \n",
    "Fay's method, can be found [here][5]. However, the essence of Fay's method\n",
    "is to use a series of different sampling weights for each observation\n",
    "(the replicate weights) to create an estimate of \n",
    "population parameter of interest, $\\sigma \\hat (\\theta \\hat)$, the standard \n",
    "error of the average HDD and CDD for each census region. The formula is:\n",
    "$$ \\hat{\\sigma} (\\hat{\\theta}) = \n",
    "\\sqrt{\\frac{1}{R(1-\\epsilon)^2}\\sum_{r=1}^R (\\hat{\\theta} \n",
    "r - \\hat{\\theta})^2} $$\n",
    "\n",
    "Where:\n",
    "* $\\hat{\\sigma}$ is the estimate of the standard error\n",
    "* $\\hat{\\theta}$ is the estimate of the population mean\n",
    "* $R$ is the total number of replicate weights ($244$ for 2009 and $96$\n",
    "for 2015)\n",
    "* $\\epsilon$ is the Fay coefficeint between $[0,1)$ and equal to $0.5$ in \n",
    "the RECS survey\n",
    "* $r$ is a given BRR replicate weight\n",
    "[5]: https://www.eia.gov/consumption/residential/methodology/2009/pdf/using-microdata-022613.pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995613d",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "We begin by saving the urls for the data sets into variables, and if the\n",
    ".csv files do not already exist in our working directory, we download them\n",
    "and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_09 = (\"https://www.eia.gov/consumption/residential\" +\n",
    "         \"/data/2009/csv/recs2009_public.csv\")\n",
    "url_weights_09 = (\"https://www.eia.gov/consumption/\" +\n",
    "                 \"residential/data/2009/csv/recs2009_public_repweights.csv\")\n",
    "url_15 = (\"https://www.eia.gov/consumption/residential\"+\n",
    "          \"/data/2015/csv/recs2015_public_v4.csv\")\n",
    "\n",
    "# Save Vars for Later\n",
    "file_dict = { \n",
    "        \"recs2009_public.csv\" : url_09,\n",
    "        \"recs2009_public_repweights.csv\"  : url_weights_09,\n",
    "        \"recs2015_public_v4.csv\" : url_15\n",
    "        }\n",
    "\n",
    "for x in file_dict.keys(): \n",
    "    if not os.exists(x): \n",
    "        pd.read_csv(file_dict[x]).to_csv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105d10a",
   "metadata": {},
   "source": [
    "We are now ready to load the variables into memory and manipulate them with\n",
    "Python. We only want to keep the variables that we will actually use, but\n",
    "first we need to generate labels for the replicate weights and extract them\n",
    "from the 2015 data set (2009 replicate weights are already separate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "brrwt = \"BRRWT\" # Label convention for 2015 data\n",
    "brr_wt = \"brr_weight_\" # Label convention for 2009 data\n",
    "wt_labels09 = [\"DOEID\"] # We need to be able to match the brr to given DOEID\n",
    "wt_labels15 = [\"DOEID\"]\n",
    "\n",
    "for i in range(1, 97): # 2015 has 96 brr\n",
    "    wt_labels15.append(brrwt + str(i))\n",
    "\n",
    "for i in range(1, 245): # 2009 has 244 brr\n",
    "    wt_labels09.append(brr_wt + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692961a6",
   "metadata": {},
   "source": [
    "    Now we can finally load the data into memory, and only select the\n",
    "columns desired via Pandas' column subsetting functionality. While we're at\n",
    "it, we will also convert `DOEID` and `REGIONC` into categorical variables; \n",
    "the rest will remain as the default float-like numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_09 = pd.read_csv(\"recs2009_public.csv\", low_memory = False)\n",
    "wts_09 = pd.read_csv(\"recs2009_public_repweights.csv\", low_memory = False)\n",
    "data_15 = pd.read_csv(\"recs2015_public_v4.csv\", low_memory = False)\n",
    "wts_15 = data_15[wt_labels15] # Saving the 2015 weights for later\n",
    "\n",
    "# Keep only the relevant variables\n",
    "keep_vars = [\n",
    "     \"DOEID\",\n",
    "     \"REGIONC\",\n",
    "     \"NWEIGHT\",\n",
    "     \"HDD65\",\n",
    "     \"CDD65\"\n",
    "    ]\n",
    "\n",
    "data_09 = data_09[keep_vars]\n",
    "data_15 = data_15[keep_vars]\n",
    "\n",
    "# Convert categorical data to appropriate type\n",
    "data_09[\"REGIONC\"] = pd.Categorical(data_09[\"REGIONC\"])\n",
    "data_15[\"REGIONC\"] = pd.Categorical(data_15[\"REGIONC\"])\n",
    "data_09[\"DOEID\"] = pd.Categorical(data_09[\"DOEID\"])\n",
    "data_15[\"DOEID\"] = pd.Categorical(data_15[\"DOEID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f6b8c",
   "metadata": {},
   "source": [
    "    The replicate weights are provided in a somewhat inconvenient format,\n",
    "with each brr weight having a separate column. We want to convert to a long\n",
    "format to make it easier to vectorize our procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0182a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_09_long = (wts_09\n",
    "               .melt(id_vars = \"DOEID\", value_vars = wt_labels09[1:245])\n",
    "               .rename(columns = {\"variable\" : \"wt_name\",\n",
    "                                  \"value\" : \"weight\"})\n",
    "               )\n",
    "\n",
    "wts_15_long = (wts_15\n",
    "               .melt(id_vars = \"DOEID\", value_vars = wt_labels15[1:97])\n",
    "               .rename(columns = {\"variable\" : \"wt_name\",\n",
    "                                  \"value\" : \"weight\"})\n",
    "               )\n",
    "\n",
    "# Add the desired variables to the long format, so that we can merge later\n",
    "wts_09 = pd.merge(wts_09_long, \n",
    "                  data_09[[\"DOEID\", \"REGIONC\", \"HDD65\", \"CDD65\"]],\n",
    "                  on=\"DOEID\",\n",
    "                  how = \"outer\")\n",
    "\n",
    "wts_15 = pd.merge(wts_15_long, \n",
    "                  data_15[[\"DOEID\", \"REGIONC\", \"HDD65\", \"CDD65\"]],\n",
    "                  on=\"DOEID\",\n",
    "                  how = \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8e1ce",
   "metadata": {},
   "source": [
    "# Construct and Report Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6f315",
   "metadata": {},
   "source": [
    "    We begin by combining the data sets with the replicate weights so that \n",
    "we can calculate the weighted average HDD and CDD and the weighted BRR\n",
    "terms in one pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a3a15",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "data_09 = (data_09\n",
    "           .assign(wt_name = \"NWEIGHT\", \n",
    "                   weight = data_09[\"NWEIGHT\"])\n",
    "           .drop(\"NWEIGHT\", axis = 1)\n",
    "           )\n",
    "\n",
    "data_15 = (data_15\n",
    "           .assign(wt_name = \"NWEIGHT\", \n",
    "                   weight = data_15[\"NWEIGHT\"])\n",
    "           .drop(\"NWEIGHT\", axis = 1)\n",
    "           )\n",
    "\n",
    "data_09 = pd.merge(data_09, \n",
    "                     wts_09, \n",
    "                     how = \"outer\", \n",
    "                     on=[\"DOEID\", \n",
    "                         \"wt_name\", \n",
    "                         \"REGIONC\",\n",
    "                         \"HDD65\", \n",
    "                         \"CDD65\", \n",
    "                         \"weight\"])\n",
    "\n",
    "data_15 = pd.merge(data_15, \n",
    "                     wts_15, \n",
    "                     how = \"outer\", \n",
    "                     on=[\"DOEID\", \n",
    "                         \"wt_name\", \n",
    "                         \"REGIONC\",\n",
    "                         \"HDD65\", \n",
    "                         \"CDD65\", \n",
    "                         \"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06483117",
   "metadata": {},
   "source": [
    "Because we want the point estimate and confidence interval of HDD and\n",
    "CDD for each of the four census regions, we can make user of the Pandas\n",
    "`groupby()` function to aggregate by type of weight (NWEIGHT vs. BRR) and\n",
    "by `REGIONC`. We introduce a helper function to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recs(x):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Pandas Data Frame with \"HDD65\", \"CDD65\" columns and \"NWEIGHT\",\n",
    "    \"brr_*\" type columns held under a \"weight\" column\n",
    "        Processes a dataframe grouped by type of weight \"wt_name\" and other\n",
    "        desired groupings, then applies a weighted average for each type of \n",
    "        weight of the \"HDD65\" and \"CDD65\" columns, respectively\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas Series Object\n",
    "        Returns a Pandas Series object for both the heating and cooling degree\n",
    "        days.\n",
    "\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'avg_heat' : (x['HDD65']* x['weight']).sum() / x[\"weight\"].sum(),\n",
    "        'avg_cool' : (x['CDD65']* x['weight']).sum() / x[\"weight\"].sum(),\n",
    "        }\n",
    "    return pd.Series(result)\n",
    "\n",
    "wavgs_09 = (data_09\n",
    "            .groupby([\"wt_name\", \"REGIONC\"])\n",
    "            .apply(process_recs))\n",
    "\n",
    "wavgs_15 = (data_15\n",
    "            .groupby([\"wt_name\", \"REGIONC\"])\n",
    "            .apply(process_recs))\n",
    "\n",
    "thetas_09 = wavgs_09.loc[\"NWEIGHT\",:]\n",
    "thetas_15 = wavgs_15.loc[\"NWEIGHT\",:]\n",
    "          \n",
    "thetas_09 = thetas_09.rename(columns = {\"avg_heat\" : \"avg_heat_t\", \n",
    "                                  \"avg_cool\" : \"avg_cool_t\"})\n",
    "    \n",
    "thetas_15 = thetas_15.rename(columns = {\"avg_heat\" : \"avg_heat_t\", \n",
    "                                  \"avg_cool\" : \"avg_cool_t\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8a2de",
   "metadata": {},
   "source": [
    "With the point estimates out of the way for both 2009 and 2015, we can move\n",
    "on to estimating the standard error with the BRR method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f563d",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "brr_09 = pd.merge(wavgs_09.iloc[4:,:], thetas_09, on=\"REGIONC\")\n",
    "brr_09[\"squared_diff_cool\"] = (brr_09[\"avg_cool\"] - brr_09[\"avg_cool_t\"])**2\n",
    "brr_09[\"squared_diff_heat\"] = (brr_09[\"avg_heat\"] - brr_09[\"avg_heat_t\"])**2\n",
    "\n",
    "brr_15 = pd.merge(wavgs_09.iloc[4:,:], thetas_15, on=\"REGIONC\")\n",
    "brr_15[\"squared_diff_cool\"] = (brr_15[\"avg_cool\"] - brr_15[\"avg_cool_t\"])**2\n",
    "brr_15[\"squared_diff_heat\"] = (brr_15[\"avg_heat\"] - brr_15[\"avg_heat_t\"])**2\n",
    "\n",
    "fay = 0.5\n",
    "\n",
    "r_09 = 244\n",
    "r_15 = 96\n",
    "\n",
    "cf_09 = (1/(r_09*(1-fay)**2))\n",
    "cf_15 = (1/(r_15*(1-fay)**2))\n",
    "\n",
    "se_est_09 = (brr_09.groupby(\"REGIONC\")\n",
    "          .sum()\n",
    "          .drop([\"avg_heat\", \n",
    "                 \"avg_cool\", \n",
    "                 \"avg_cool_t\", \n",
    "                 \"avg_heat_t\"],axis = 1)\n",
    "          .multiply(cf_09)\n",
    "          .pow(0.5)\n",
    "          .rename(columns = {\"squared_diff_cool\" : \"std_error_cool\",\n",
    "                             \"squared_diff_heat\" : \"std_error_heat\"})\n",
    "          )\n",
    "\n",
    "se_est_15 = (brr_15.groupby(\"REGIONC\")\n",
    "          .sum()\n",
    "          .drop([\"avg_heat\", \n",
    "                 \"avg_cool\", \n",
    "                 \"avg_cool_t\", \n",
    "                 \"avg_heat_t\"],axis = 1)\n",
    "          .multiply(cf_15)\n",
    "          .pow(0.5)\n",
    "          .rename(columns = {\"squared_diff_cool\" : \"std_error_cool\",\n",
    "                             \"squared_diff_heat\" : \"std_error_heat\"})\n",
    "          )\n",
    "\n",
    "\n",
    "stats_09 = pd.merge(thetas_09, se_est_09, on=\"REGIONC\").reset_index()\n",
    "stats_15 = pd.merge(thetas_15, se_est_15, on=\"REGIONC\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a25756",
   "metadata": {},
   "source": [
    "At long last, we are able to consolidate our data into a nicely-formatted\n",
    "table after we have calculated the 95% confidence interval on our point\n",
    "estimates of heating and cooling degree days for each region in 2009 \n",
    "and 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_est_bounds(u ,se, cl = 0.95, diff = False):\n",
    "    \"\"\"\n",
    "    Constructs a confidence interval estimate based on normal theory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : float, required\n",
    "        Point estimate passed as argument\n",
    "    se : float, required\n",
    "        Standard Error passed for calulating confidence interval\n",
    "    cl : float, optional\n",
    "        DESCRIPTION. The default is 0.95.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tupel(float)\n",
    "        A tuple of (lwr bound, upr bound)\n",
    "    \"\"\"\n",
    "    alpha = (1 - cl)/2 # CIs are always two-tailed\n",
    "    z = stats.norm.ppf(1-alpha)\n",
    "    \n",
    "    lower, upper = u - z*se, u + z*se \n",
    "    if(not diff): # is this a CI for a difference \n",
    "        lower = lower.clip(lower=0) # CDD / HDD can never be negatives\n",
    "        \n",
    "    return lower, upper\n",
    "\n",
    "def prep_table(stats_df, diff):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats_df : Pandas DataFrame with required fields\n",
    "        Creates 95% confidence interval, renames labels,\n",
    "        and reorders columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Processed / prepped stats table as Pandas DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    regions = {1: \"Northeast\", 2: \"Midwest\", 3: \"South\", 4: \"West\"}\n",
    "\n",
    "    stats_df[\"REGIONC\"] = stats_df[\"REGIONC\"].map(regions)\n",
    "    stats_df = stats_df.rename(columns = {\"REGIONC\" : \"Region\"})\n",
    "    stats_df.set_index(\"Region\")\n",
    "    \n",
    "    heat_bounds = std_est_bounds(stats_df[\"avg_heat_t\"],\n",
    "                             stats_df[\"std_error_heat\"],\n",
    "                             diff=diff)\n",
    "    cool_bounds = std_est_bounds(stats_df[\"avg_cool_t\"], \n",
    "                             stats_df[\"std_error_cool\"],\n",
    "                             diff=diff)\n",
    "    \n",
    "    stats_df[\"heat_lwr\"], stats_df[\"heat_upr\"] = heat_bounds\n",
    "    stats_df[\"cool_lwr\"], stats_df[\"cool_upr\"] = cool_bounds\n",
    "\n",
    "    ordered_cols = [\"Region\",\n",
    "                \"avg_heat_t\", \n",
    "                \"std_error_heat\", \n",
    "                \"heat_lwr\", \n",
    "                \"heat_upr\",\n",
    "                \"avg_cool_t\",\n",
    "                \"std_error_cool\",\n",
    "                \"cool_lwr\",\n",
    "                \"cool_upr\"]\n",
    "    \n",
    "    stats_df = stats_df[ordered_cols]\n",
    "    col_labels = {\"avg_heat_t\" : \"Avg Heating Degree Days\",\n",
    "               \"std_error_heat\" : \"SE Heating Degree Days\",\n",
    "               \"heat_lwr\" : \"Lower Bound CI Heating Degree Days\",\n",
    "               \"heat_upr\" : \"Upper Bound CI Heating Degree Days\",\n",
    "               \"avg_cool_t\": \"Avg Cooling Degree Days\", \n",
    "               \"std_error_cool\" : \"SE Cooling Degree Days\",\n",
    "               \"cool_lwr\" : \"Lower Bound CI Cooling Degree Days\",\n",
    "               \"cool_upr\" : \"Upper Bound CI Cooling Degree Days\",\n",
    "             }\n",
    "    \n",
    "    return stats_df.rename(columns = col_labels).set_index(\"Region\")\n",
    "\n",
    "table_df_09 = prep_table(stats_09, False)\n",
    "table_df_15 = prep_table(stats_15, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce97f9",
   "metadata": {},
   "source": [
    "### 2009 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption09 = \"\"\"\n",
    "<b> 2009 Heating and Cooling </b> <em> Average number of heating & cooling \n",
    "degree days by Census region, 2009. Estimates generated via final \n",
    "sample weights, and standard errors estimated by the BRR (Fay) method</em>\n",
    "\"\"\"\n",
    "t_09 = table_df_09.to_html(index=True, \n",
    "                                 float_format = lambda x: \"%10.0f\" % x)\n",
    "t_09 = t_09.rsplit('\\n')\n",
    "t_09.insert(1, caption09)\n",
    "table_09 = ''\n",
    "for i, line in enumerate(t_09):\n",
    "    table_09 += line\n",
    "    if i < (len(t_09) - 1):\n",
    "        table_09 += '\\n'\n",
    "        \n",
    "display(HTML(table_09))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b369c",
   "metadata": {},
   "source": [
    "### 2015 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b98697",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption15 = \"\"\"\n",
    "<b> 2015 Heating and Cooling </b> <em> Average number of heating & cooling \n",
    "degree days by Census region, 2015. Estimates generated via final \n",
    "sample weights, and standard errors estimated by the BRR (Fay) method</em>\n",
    "\"\"\"\n",
    "t_15 = table_df_15.to_html(index=True, \n",
    "                                 float_format = lambda x: \"%10.0f\" % x)\n",
    "t_15 = t_15.rsplit('\\n')\n",
    "t_15.insert(1, caption15)\n",
    "table_15 = ''\n",
    "for i, line in enumerate(t_15):\n",
    "    table_15 += line\n",
    "    if i < (len(t_15) - 1):\n",
    "        table_15 += '\\n'\n",
    "        \n",
    "display(HTML(table_15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26147748",
   "metadata": {},
   "source": [
    "    Our next task is to leverage our work from the previous tables to \n",
    "estimate the *change* in heating and cooling degree days from 2009 to 2015. \n",
    "By using the fact that these two measures are independent, we can find the \n",
    "variance of the difference as the sum of the variance for each corresponding \n",
    "estimate from both survey years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_thetas = thetas_15 - thetas_09\n",
    "diff_se = (se_est_15.pow(2) + se_est_09.pow(2)).pow(0.5)\n",
    "stats_diff = pd.merge(diff_thetas, diff_se, on=\"REGIONC\").reset_index()\n",
    "table_df_diff = prep_table(stats_diff, True)\n",
    "\n",
    "captiondiff = \"\"\"\n",
    "<b> Difference in Heating and Cooling, 2009 - 2015 </b> \n",
    "<em> Difference in average number of heating & cooling degree days by \n",
    "Census region, between 2009 and 2015. Estimates generated via final \n",
    "sample weights, and\n",
    "standard errors estimated by the BRR (Fay) method</em>\n",
    "\"\"\"\n",
    "t_diff = table_df_diff.to_html(index=True, \n",
    "                                 float_format = lambda x: \"%10.0f\" % x)\n",
    "t_diff = t_diff.rsplit('\\n')\n",
    "t_diff.insert(1, captiondiff)\n",
    "table_diff = ''\n",
    "for i, line in enumerate(t_diff):\n",
    "    table_diff += line\n",
    "    if i < (len(t_diff) - 1):\n",
    "        table_diff += '\\n'\n",
    "        \n",
    "display(HTML(table_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71297f8",
   "metadata": {},
   "source": [
    "# Plot Estimates / CI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc0904",
   "metadata": {},
   "source": [
    "To make the data more easy to visualize, we can display the data in a plot\n",
    "of point estimates with the corresponding confidence intervals as error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df_09 = table_df_09.reset_index()\n",
    "table_df_15 = table_df_15.reset_index()\n",
    "table_df_diff = table_df_diff.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_09 = plt.figure()\n",
    "ax09_1 = fig_09.add_subplot()\n",
    "\n",
    "ax09_1.set_ylabel(\"Avg Degree Days\")\n",
    "ax09_1.set_title(\"Avg CDD / HDD in 2009 by Region, 95% CI\")\n",
    "\n",
    "bars = ax09_1.scatter(\n",
    "    x=table_df_09[\"Region\"],\n",
    "    y=table_df_09[\"Avg Cooling Degree Days\"],\n",
    "    marker= \"s\",\n",
    "    color=\"blue\",\n",
    "    label=\"Avg CDD\"\n",
    "    )\n",
    "\n",
    "bars2 = ax09_1.scatter(\n",
    "    x=table_df_09[\"Region\"],\n",
    "    y=table_df_09[\"Avg Heating Degree Days\"],\n",
    "    marker= \"s\",\n",
    "    color=\"red\",\n",
    "    label= \"Avg HDD\"\n",
    "    )\n",
    "\n",
    "fig_09.legend(bbox_to_anchor = (0.95, 0.95), loc = \"upper left\")\n",
    "\n",
    "_= plt.errorbar(\n",
    "    data=table_df_09,\n",
    "    x = \"Region\",\n",
    "    y = \"Avg Cooling Degree Days\",\n",
    "    fmt=\"None\",\n",
    "    yerr = \"SE Cooling Degree Days\",\n",
    "    ecolor = \"blue\",\n",
    "    capsize = 4\n",
    "    )\n",
    "\n",
    "_= plt.errorbar(\n",
    "    data=table_df_09,\n",
    "    x = \"Region\",\n",
    "    y = \"Avg Heating Degree Days\",\n",
    "    fmt=\"None\",\n",
    "    yerr = \"SE Heating Degree Days\",\n",
    "    ecolor = \"red\",\n",
    "    capsize = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604759f",
   "metadata": {},
   "source": [
    "Shown above is a plot of the 2009 estimated HDD / CDD by region; the\n",
    "large number of replicate weights creates very narrow confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e185364",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_15 = plt.figure()\n",
    "ax15 = fig_15.add_subplot()\n",
    "\n",
    "ax15.set_ylabel(\"Avg Degree Days\")\n",
    "ax15.set_title(\"Avg CDD / HDD in 2015 by Region, 95% CI\")\n",
    "\n",
    "bars3 = ax15.scatter(\n",
    "    x=table_df_15[\"Region\"],\n",
    "    y=table_df_15[\"Avg Cooling Degree Days\"],\n",
    "    marker= \"s\",\n",
    "    color=\"blue\",\n",
    "    label=\"Avg CDD\"\n",
    "    )\n",
    "\n",
    "bars4 = ax15.scatter(\n",
    "    x=table_df_15[\"Region\"],\n",
    "    y=table_df_15[\"Avg Heating Degree Days\"],\n",
    "    marker= \"s\",\n",
    "    color=\"red\",\n",
    "    label=\"Avg HDD\"\n",
    "    )\n",
    "\n",
    "fig_15.legend(bbox_to_anchor = (0.95, 0.95), loc = \"upper left\")\n",
    "\n",
    "_= plt.errorbar(\n",
    "    data=table_df_15,\n",
    "    x = \"Region\",\n",
    "    y = \"Avg Cooling Degree Days\",\n",
    "    fmt=\"None\",\n",
    "    yerr = \"SE Cooling Degree Days\",\n",
    "    ecolor = \"blue\",\n",
    "    capsize = 4\n",
    "    )\n",
    "\n",
    "_= plt.errorbar(\n",
    "    data=table_df_15,\n",
    "    x = \"Region\",\n",
    "    y = \"Avg Heating Degree Days\",\n",
    "    fmt=\"None\",\n",
    "    yerr = \"SE Heating Degree Days\",\n",
    "    ecolor = \"red\",\n",
    "    capsize = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ebd95",
   "metadata": {},
   "source": [
    "As the 2015 data has fewer replicate weights, the confidence intervals are\n",
    "considerably larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537256f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_diff = plt.figure()\n",
    "\n",
    "axd = fig_diff.add_subplot()\n",
    "axd.set_ylabel(\"Avg Diff Degree Days\")\n",
    "axd.set_title(\"Avg Diff in CDD / HDD 2009 to 2015 by Region, 95% CI\")\n",
    "\n",
    "bars5 = axd.scatter(\n",
    "    x=table_df_diff[\"Region\"],\n",
    "    y=table_df_diff[\"Avg Cooling Degree Days\"],\n",
    "    marker= \"s\",\n",
    "    color=\"blue\",\n",
    "    label=\"Avg Diff in CDD\"\n",
    "    )\n",
    "\n",
    "bars6 = axd.scatter(\n",
    "    x=table_df_diff[\"Region\"],\n",
    "    y=table_df_diff[\"Avg Heating Degree Days\"],\n",
    "    marker= \"s\",\n",
    "    color=\"red\",\n",
    "    label=\"Avg Diff in HDD\"\n",
    "    )\n",
    "\n",
    "fig_diff.legend(bbox_to_anchor = (0.95, 0.95), loc = \"upper left\")\n",
    "\n",
    "_= plt.errorbar(\n",
    "    data=table_df_diff,\n",
    "    x = \"Region\",\n",
    "    y = \"Avg Cooling Degree Days\",\n",
    "    fmt=\"None\",\n",
    "    yerr = \"SE Cooling Degree Days\",\n",
    "    ecolor = \"blue\",\n",
    "    capsize = 4\n",
    "    )\n",
    "\n",
    "_= plt.errorbar(\n",
    "    data=table_df_diff,\n",
    "    x = \"Region\",\n",
    "    y = \"Avg Heating Degree Days\",\n",
    "    fmt=\"None\",\n",
    "    yerr = \"SE Heating Degree Days\",\n",
    "    ecolor = \"red\",\n",
    "    capsize = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f63ac",
   "metadata": {},
   "source": [
    "Because the variance is additive for the difference between two independent\n",
    "variables, the confidence intervals for the difference in CDD / HDD from \n",
    "2009 to 2015 are the largest we've seen yet."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
